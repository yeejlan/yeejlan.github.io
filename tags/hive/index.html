<!DOCTYPE html><html><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="alternative" href="/atom.xml" title="Yee's 部落格" type="application/atom+xml"><link rel="icon" href="/favicon.png"><title>hive - Yee's 部落格</title><link rel="stylesheet" href="/css/main.css" type="text/css"><!--[if lt IE 9]><script>(function(a,b){a="abbr article aside audio bdi canvas data datalist details dialog figcaption figure footer header hgroup main mark meter nav output progress section summary template time video".split(" ");for(b=a.length-1;b>=0;b--)document.createElement(a[b])})()</script><![endif]--></head><body><header class="head"><h1 class="head-title u-fl"><a href="/">Yee's 部落格</a></h1><nav class="head-nav u-fr"><ul class="head-nav__list"><li class="head-nav__item"><a href="/" class="head-nav__link">Home</a></li><li class="head-nav__item"><a href="/archives" class="head-nav__link">Archives</a></li></ul></nav></header><main class="main"><article class="post"><header class="post__head"><time datetime="2014-07-31T02:40:00.000Z" class="post__time">July 31, 2014</time><h1 class="post__title"><a href="/2014/07/31/hadoop-hive-pig-development-environment-setup/">Hadoop,hive,pig开发环境搭建</a></h1></header><div class="post__main echo"><p>Hadoop的版本推荐选用cloudera的CDH系列，因为这个系列把hadoop,hive,pig等工具的版本已经匹配好了，并且有详细的安装说明文档，细节见<a href="http://www.cloudera.com/content/support/en/documentation.html" target="_blank" rel="external">http://www.cloudera.com/content/support/en/documentation.html</a></p>
<p>目前最新的发布版是<a href="http://www.cloudera.com/content/support/en/documentation/cdh5-documentation/cdh5-documentation-v5-latest.html" target="_blank" rel="external">CDH 5</a>，支持的操作系统有CentOS6.5和Ubuntu12.04，搭建hadoop开发环境我更偏向centos6.5些。</p>
<p>首先安装JDK，官方推荐使用Oracle JDK，不建议使用OpenJDK。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
2
3
</pre></td><td class="code"><pre>wget http://download.oracle.com/otn-pub/java/jdk/<span class="number">8</span>u11-b12/jdk-<span class="number">8</span>u11-linux-x64.tar.gz
tar xvzf jdk-<span class="number">8</span>u11-linux-x64.tar.gz
<span class="built_in">sudo</span> mv jdk1.<span class="number">8.0</span>_11/ /opt/
</pre></td></tr></table></figure>

<p>配置JAVA_HOME和PATH</p>
<figure class="highlight config"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
</pre></td><td class="code"><pre><span class="comment">#sudo vi /etc/environment</span>

<span class="keyword">export</span> JAVA_HOME=/opt/jdk1.<span class="number">8.0</span>_11

<span class="comment">#sudo vi /etc/profile</span>

<span class="keyword">export</span> JAVA_HOME=/opt/jdk1.<span class="number">8.0</span>_11
<span class="keyword">export</span> PATH=<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$PATH</span>
</pre></td></tr></table></figure>

<p>下载CDH5 <a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/CDH5-Installation-Guide/cdh5ig_cdh5_install.html" target="_blank" rel="external">1-click安装包</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
2
</pre></td><td class="code"><pre>wget http://archive.cloudera.com/cdh5/one-click-install/redhat/<span class="number">6</span>/x86_64/cloudera-cdh-<span class="number">5</span>-<span class="number">0</span>.x86_64.rpm
<span class="built_in">sudo</span> yum --nogpgcheck localinstall cloudera-cdh-<span class="number">5</span>-<span class="number">0</span>.x86_64.rpm
</pre></td></tr></table></figure><br>安装上面的包会添加cloudera的仓库配置，查询包所安装的文件列表如下<br><br><figure class="highlight bash"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
</pre></td><td class="code"><pre>rpm -lq cloudera-cdh-<span class="number">5</span>-<span class="number">0</span>.x86_64
/etc/pki/rpm-gpg
/etc/pki/rpm-gpg/RPM-GPG-KEY-cloudera
/etc/yum.repos.d/cloudera-cdh5.repo     <span class="comment">#仓库配置文件</span>
/usr/share/doc/cloudera-cdh-<span class="number">5</span>
/usr/share/doc/cloudera-cdh-<span class="number">5</span>/LICENSE
</pre></td></tr></table></figure>

<p>下面安装Hadoop, 使用第二代的yarn作为mapper/reducer调度器</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre><span class="built_in">sudo</span> yum install hadoop-conf-pseudo  <span class="comment">#这个是开发环境的配置文件，它的依赖里面有hadoop hdfs+yarn</span>
</pre></td></tr></table></figure>

<p>把主机名加入hosts列表</p>
<figure class="highlight config"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre><span class="number">127.0</span>.<span class="number">0.1</span>  vm4   <span class="comment">#主机名是vm4</span>
</pre></td></tr></table></figure>

<p>格式化NameNode</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre><span class="built_in">sudo</span> -u hdfs hdfs namenode -format
</pre></td></tr></table></figure>

<p>给hadoop启动脚本添加JAVA_HOME环境变量</p>
<figure class="highlight config"><table><tr><td class="gutter"><pre>1
2
3
</pre></td><td class="code"><pre><span class="comment">#sudo vi /etc/hadoop/conf/hadoop-env.sh</span>

<span class="keyword">export</span> JAVA_HOME=/opt/jdk1.<span class="number">8.0</span>_11
</pre></td></tr></table></figure>

<p>启动hadoop namenode,datanode,secondarynamenode服务</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre><span class="keyword">for</span> x <span class="keyword">in</span> `<span class="built_in">cd</span> /etc/init.d ; ls hadoop-hdfs-*` ; <span class="keyword">do</span> <span class="built_in">sudo</span> service <span class="variable">$x</span> stop ; <span class="keyword">done</span>
</pre></td></tr></table></figure>

<p>创建hdfs相关的目录结构</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
</pre></td><td class="code"><pre><span class="built_in">sudo</span> -u hdfs hadoop fs -rm -r /tmp  <span class="comment">#确保/tmp目录不存在</span>

<span class="built_in">sudo</span> -u hdfs hadoop fs -mkdir -p /tmp/hadoop-yarn/staging/history/<span class="keyword">done</span>_intermediate
<span class="built_in">sudo</span> -u hdfs hadoop fs -chown -R mapred:mapred /tmp/hadoop-yarn/staging 
<span class="built_in">sudo</span> -u hdfs hadoop fs -chmod -R <span class="number">1777</span> /tmp 
<span class="built_in">sudo</span> -u hdfs hadoop fs -mkdir -p /var/log/hadoop-yarn
<span class="built_in">sudo</span> -u hdfs hadoop fs -chown yarn:mapred /var/log/hadoop-yarn
</pre></td></tr></table></figure>

<p>验证下创建过的目录结构</p>
<figure class="highlight config"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
</pre></td><td class="code"><pre>sudo <span class="attribute">-u</span> hdfs hadoop fs <span class="attribute">-ls</span> <span class="attribute">-R</span> <span class="subst">/</span>

drwxrwxrwt   <span class="subst">-</span> hdfs supergroup          <span class="number">0</span> <span class="number">2014</span><span class="subst">-</span><span class="number">07</span><span class="subst">-</span><span class="number">31</span> <span class="number">16</span>:<span class="number">18</span> /tmp
drwxrwxrwt   <span class="subst">-</span> hdfs supergroup          <span class="number">0</span> <span class="number">2014</span><span class="subst">-</span><span class="number">07</span><span class="subst">-</span><span class="number">31</span> <span class="number">16</span>:<span class="number">18</span> /tmp/hadoop<span class="attribute">-yarn</span>
drwxrwxrwt   <span class="subst">-</span> mapred mapred              <span class="number">0</span> <span class="number">2014</span><span class="subst">-</span><span class="number">07</span><span class="subst">-</span><span class="number">31</span> <span class="number">16</span>:<span class="number">18</span> /tmp/hadoop<span class="attribute">-yarn</span>/staging
drwxrwxrwt   <span class="subst">-</span> mapred mapred              <span class="number">0</span> <span class="number">2014</span><span class="subst">-</span><span class="number">07</span><span class="subst">-</span><span class="number">31</span> <span class="number">16</span>:<span class="number">18</span> /tmp/hadoop<span class="attribute">-yarn</span>/staging/history
drwxrwxrwt   <span class="subst">-</span> mapred mapred              <span class="number">0</span> <span class="number">2014</span><span class="subst">-</span><span class="number">07</span><span class="subst">-</span><span class="number">31</span> <span class="number">16</span>:<span class="number">18</span> /tmp/hadoop<span class="attribute">-yarn</span>/staging/history/done_intermediate
drwxr<span class="attribute">-xr</span><span class="attribute">-x</span>   <span class="subst">-</span> hdfs   supergroup          <span class="number">0</span> <span class="number">2014</span><span class="subst">-</span><span class="number">07</span><span class="subst">-</span><span class="number">31</span> <span class="number">16</span>:<span class="number">18</span> /<span class="built_in">var</span>
drwxr<span class="attribute">-xr</span><span class="attribute">-x</span>   <span class="subst">-</span> hdfs   supergroup          <span class="number">0</span> <span class="number">2014</span><span class="subst">-</span><span class="number">07</span><span class="subst">-</span><span class="number">31</span> <span class="number">16</span>:<span class="number">18</span> /<span class="built_in">var</span>/<span class="keyword">log</span>
drwxr<span class="attribute">-xr</span><span class="attribute">-x</span>   <span class="subst">-</span> yarn   mapred              <span class="number">0</span> <span class="number">2014</span><span class="subst">-</span><span class="number">07</span><span class="subst">-</span><span class="number">31</span> <span class="number">16</span>:<span class="number">18</span> /<span class="built_in">var</span>/<span class="keyword">log</span>/hadoop<span class="attribute">-yarn</span>
</pre></td></tr></table></figure>

<p>启动map/reduce相关服务</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
2
3
</pre></td><td class="code"><pre><span class="built_in">sudo</span> service hadoop-yarn-resourcemanager start 
<span class="built_in">sudo</span> service hadoop-yarn-nodemanager start 
<span class="built_in">sudo</span> service hadoop-mapreduce-historyserver start
</pre></td></tr></table></figure>

<p>产生用户目录，在hdfs下，每个用户有自己的home目录，我的测试机的用户名叫jojo，因此</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
2
</pre></td><td class="code"><pre><span class="built_in">sudo</span> -u hdfs hadoop fs -mkdir -p /user/jojo
<span class="built_in">sudo</span> -u hdfs hadoop fs -chown jojo /user/jojo
</pre></td></tr></table></figure>

<p>到目前为止hadoop hdfs+yarn map/reduce调度器安装完成了，下面测试下:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
</pre></td><td class="code"><pre>hadoop fs -mkdir input <span class="comment">#创建一个input目录</span>
hadoop fs -put /etc/hadoop/conf/*.xml input  <span class="comment">#把hadoop的配置文件copy到input目录</span>
hadoop fs -ls input   <span class="comment">#列出input目录下的文件</span>

Found <span class="number">4</span> items
-rw-r--r--   <span class="number">1</span> jojo supergroup       <span class="number">2133</span> <span class="number">2014</span>-<span class="number">07</span>-<span class="number">31</span> <span class="number">16</span>:<span class="number">28</span> input/core-site.xml
-rw-r--r--   <span class="number">1</span> jojo supergroup       <span class="number">2324</span> <span class="number">2014</span>-<span class="number">07</span>-<span class="number">31</span> <span class="number">16</span>:<span class="number">28</span> input/hdfs-site.xml
-rw-r--r--   <span class="number">1</span> jojo supergroup       <span class="number">1549</span> <span class="number">2014</span>-<span class="number">07</span>-<span class="number">31</span> <span class="number">16</span>:<span class="number">28</span> input/mapred-site.xml
-rw-r--r--   <span class="number">1</span> jojo supergroup       <span class="number">2375</span> <span class="number">2014</span>-<span class="number">07</span>-<span class="number">31</span> <span class="number">16</span>:<span class="number">28</span> input/yarn-site.xml
</pre></td></tr></table></figure>

<p>编辑.bashrc设置HADOOP_MAPRED_HOME, 这个变量在运行map/reduce程序的时候需要用到</p>
<figure class="highlight config"><table><tr><td class="gutter"><pre>1
2
3
4
5
</pre></td><td class="code"><pre><span class="comment">#vi ~/.bashrc</span>

<span class="keyword">export</span> HADOOP_MAPRED_HOME=/usr/lib/hadoop-mapreduce  <span class="comment">#加入这行</span>

<span class="built_in">source</span> ~/.bashrc  <span class="comment">#source下，让export立即生效</span>
</pre></td></tr></table></figure>

<p>运行例子程序</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
2
</pre></td><td class="code"><pre><span class="comment">#把input下符合'dfs[a-z.]+'的项输出到output23目录下</span>
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar grep input output23 <span class="string">'dfs[a-z.]+'</span>
</pre></td></tr></table></figure><br>列出 output23目录看看<br><br><figure class="highlight config"><table><tr><td class="gutter"><pre>1
2
3
4
5
</pre></td><td class="code"><pre>hadoop fs <span class="attribute">-ls</span> output23 

Found <span class="number">2</span> items
<span class="attribute">-rw</span><span class="attribute">-r</span><span class="subst">--</span>r<span class="subst">--</span>   <span class="number">1</span> jojo supergroup          <span class="number">0</span> <span class="number">2014</span><span class="subst">-</span><span class="number">07</span><span class="subst">-</span><span class="number">31</span> <span class="number">16</span>:<span class="number">35</span> output23/_SUCCESS
<span class="attribute">-rw</span><span class="attribute">-r</span><span class="subst">--</span>r<span class="subst">--</span>   <span class="number">1</span> jojo supergroup        <span class="number">244</span> <span class="number">2014</span><span class="subst">-</span><span class="number">07</span><span class="subst">-</span><span class="number">31</span> <span class="number">16</span>:<span class="number">35</span> output23/part<span class="attribute">-r</span><span class="subst">-</span><span class="number">00000</span>
</pre></td></tr></table></figure><br>看下输出文件的内容<br><br><figure class="highlight config"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="code"><pre>hadoop fs -cat output23/part-r-<span class="number">00000</span> | head

<span class="number">1</span>       dfs<span class="preprocessor">.safemode</span><span class="preprocessor">.min</span><span class="preprocessor">.datanodes</span>
<span class="number">1</span>       dfs<span class="preprocessor">.safemode</span><span class="preprocessor">.extension</span>
<span class="number">1</span>       dfs<span class="preprocessor">.replication</span>
<span class="number">1</span>       dfs<span class="preprocessor">.namenode</span><span class="preprocessor">.name</span><span class="preprocessor">.dir</span>
<span class="number">1</span>       dfs<span class="preprocessor">.namenode</span><span class="preprocessor">.checkpoint</span><span class="preprocessor">.dir</span>
<span class="number">1</span>       dfs<span class="preprocessor">.domain</span><span class="preprocessor">.socket</span><span class="preprocessor">.path</span>
<span class="number">1</span>       dfs<span class="preprocessor">.datanode</span><span class="preprocessor">.hdfs</span>
<span class="number">1</span>       dfs<span class="preprocessor">.datanode</span><span class="preprocessor">.data</span><span class="preprocessor">.dir</span>
<span class="number">1</span>       dfs<span class="preprocessor">.client</span><span class="preprocessor">.read</span><span class="preprocessor">.shortcircuit</span>
<span class="number">1</span>       dfs<span class="preprocessor">.client</span><span class="preprocessor">.file</span>
</pre></td></tr></table></figure>

<p>到这里hadoop已经安装成功了，下面安装hive</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre><span class="built_in">sudo</span> yum install hive hive-metastore hive-server2
</pre></td></tr></table></figure><br>hive的metastore是用来存储hive的表结构的，习惯的做法是把metastore存放到mysql里面，hive-server2是hive-server的改良版，增强了并发性。<br><br>首先安装mysql<br><br><figure class="highlight bash"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td><td class="code"><pre><span class="built_in">sudo</span> yum install mysql-server

<span class="built_in">sudo</span> service mysqld start <span class="comment">#启动mysql服务</span>

<span class="comment">#安装mysql jdbc驱动</span>
<span class="built_in">sudo</span> yum install mysql-connector-java
<span class="built_in">sudo</span> ln <span class="operator">-s</span> /usr/share/java/mysql-connector-java.jar /usr/lib/hive/lib/mysql-connector-java.jar

<span class="comment">#设置mysql root用户的密码</span>
<span class="built_in">sudo</span> /usr/bin/mysql_secure_installation

<span class="comment">#确保mysql开机启动</span>
<span class="built_in">sudo</span> /sbin/chkconfig mysqld on
<span class="built_in">sudo</span> /sbin/chkconfig --list mysqld
</pre></td></tr></table></figure><br>下面产生metastore需要用的schema和用户<br><br><figure class="highlight config"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="code"><pre>mysql -u root -p
Enter password:
mysql&gt; <span class="keyword">CREATE</span> DATABASE metastore;
mysql&gt; USE metastore;
mysql&gt; SOURCE /usr/lib/hive/scripts/metastore/upgrade/mysql/hive-schema-<span class="number">0.12</span>.<span class="number">0</span>.mysql.sql;  #导入schema

#添加hive用户
mysql&gt; <span class="keyword">CREATE</span> USER <span class="string">'hive'</span>@<span class="string">'localhost'</span> IDENTIFIED <span class="keyword">BY</span> <span class="string">'passwd1234'</span>;  #密码passwd1234
mysql&gt; REVOKE ALL PRIVILEGES, GRANT OPTION <span class="keyword">FROM</span> <span class="string">'hive'</span>@<span class="string">'localhost'</span>;
mysql&gt; GRANT <span class="keyword">SELECT</span>,INSERT,UPDATE,DELETE,LOCK TABLES,EXECUTE <span class="keyword">ON</span> metastore.* <span class="keyword">TO</span> <span class="string">'hive'</span>@<span class="string">'localhost'</span>;
mysql&gt; FLUSH PRIVILEGES;
mysql&gt; quit;
</pre></td></tr></table></figure>

<p>编辑hive配置文件，加入如下配置</p>
<figure class="highlight config"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
</pre></td><td class="code"><pre>#sudo vi /etc/hive/conf/hive-site.xml

<span class="tag">&lt;<span class="title">property</span>&gt;</span>
	<span class="tag">&lt;<span class="title">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="title">name</span>&gt;</span>
	<span class="tag">&lt;<span class="title">value</span>&gt;</span>jdbc:mysql://localhost/metastore<span class="tag">&lt;/<span class="title">value</span>&gt;</span>
	<span class="tag">&lt;<span class="title">description</span>&gt;</span>the URL of the MySQL database<span class="tag">&lt;/<span class="title">description</span>&gt;</span>
<span class="tag">&lt;/<span class="title">property</span>&gt;</span>

<span class="tag">&lt;<span class="title">property</span>&gt;</span>
	<span class="tag">&lt;<span class="title">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="title">name</span>&gt;</span>
	<span class="tag">&lt;<span class="title">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="title">value</span>&gt;</span>
<span class="tag">&lt;/<span class="title">property</span>&gt;</span>

<span class="tag">&lt;<span class="title">property</span>&gt;</span>
	<span class="tag">&lt;<span class="title">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="title">name</span>&gt;</span>
	<span class="tag">&lt;<span class="title">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="title">value</span>&gt;</span>
<span class="tag">&lt;/<span class="title">property</span>&gt;</span>

<span class="tag">&lt;<span class="title">property</span>&gt;</span>
	<span class="tag">&lt;<span class="title">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="title">name</span>&gt;</span>
	<span class="tag">&lt;<span class="title">value</span>&gt;</span>passwd1234<span class="tag">&lt;/<span class="title">value</span>&gt;</span>
<span class="tag">&lt;/<span class="title">property</span>&gt;</span>

<span class="tag">&lt;<span class="title">property</span>&gt;</span>
	<span class="tag">&lt;<span class="title">name</span>&gt;</span>datanucleus.autoCreateSchema<span class="tag">&lt;/<span class="title">name</span>&gt;</span>
	<span class="tag">&lt;<span class="title">value</span>&gt;</span>false<span class="tag">&lt;/<span class="title">value</span>&gt;</span>
<span class="tag">&lt;/<span class="title">property</span>&gt;</span>

<span class="tag">&lt;<span class="title">property</span>&gt;</span>
	<span class="tag">&lt;<span class="title">name</span>&gt;</span>datanucleus.fixedDatastore<span class="tag">&lt;/<span class="title">name</span>&gt;</span>
	<span class="tag">&lt;<span class="title">value</span>&gt;</span>true<span class="tag">&lt;/<span class="title">value</span>&gt;</span>
<span class="tag">&lt;/<span class="title">property</span>&gt;</span>

<span class="tag">&lt;<span class="title">property</span>&gt;</span>
	<span class="tag">&lt;<span class="title">name</span>&gt;</span>datanucleus.autoStartMechanism<span class="tag">&lt;/<span class="title">name</span>&gt;</span>
	<span class="tag">&lt;<span class="title">value</span>&gt;</span>SchemaTable<span class="tag">&lt;/<span class="title">value</span>&gt;</span>
<span class="tag">&lt;/<span class="title">property</span>&gt;</span>

<span class="tag">&lt;<span class="title">property</span>&gt;</span>
	<span class="tag">&lt;<span class="title">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="title">name</span>&gt;</span>
	<span class="tag">&lt;<span class="title">value</span>&gt;</span>thrift://localhost:9083<span class="tag">&lt;/<span class="title">value</span>&gt;</span>
	<span class="tag">&lt;<span class="title">description</span>&gt;</span>IP address (or fully-qualified domain name) and port of the metastore host<span class="tag">&lt;/<span class="title">description</span>&gt;</span>
<span class="tag">&lt;/<span class="title">property</span>&gt;</span>
</pre></td></tr></table></figure>

<p>启动metastore服务</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
2
</pre></td><td class="code"><pre><span class="built_in">sudo</span> service hive-metastore start
<span class="built_in">sudo</span> service hive-metastore status <span class="comment">#检查下状态，因为配置项比较多，容易出错，出错后检查/var/log/hive/hive-metastore.out对照日志排错</span>
</pre></td></tr></table></figure>

<p>测试下hive是否能连接到metastore</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
2
3
4
5
</pre></td><td class="code"><pre>hive <span class="operator">-e</span> <span class="string">'show tables;'</span>

<span class="comment">#没表输出，因为还没创建过</span>
OK
Time taken: <span class="number">2.836</span> seconds
</pre></td></tr></table></figure>

<p>下面配置hive-server2, 这个服务依赖zookeeper，因此先在本机部署一个zookeeper测试实例</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
</pre></td><td class="code"><pre><span class="built_in">sudo</span> yum install zookeeper zookeeper-server

<span class="comment">#产生zookeeper的数据目录</span>
<span class="built_in">sudo</span> mkdir -p /var/lib/zookeeper
<span class="built_in">sudo</span> chown -R zookeeper /var/lib/zookeeper/

<span class="built_in">sudo</span> service zookeeper-server init <span class="comment">#初始化zookeeper</span>
<span class="built_in">sudo</span> service zookeeper-server start <span class="comment">#启动zookeeper服务</span>
</pre></td></tr></table></figure>

<p>编辑hive配置文件，增加如下配置</p>
<figure class="highlight config"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="code"><pre>#sudo vi /etc/hive/conf/hive-site.xml
<span class="tag">&lt;<span class="title">property</span>&gt;</span>
	<span class="tag">&lt;<span class="title">name</span>&gt;</span>hive.support.concurrency<span class="tag">&lt;/<span class="title">name</span>&gt;</span>
	<span class="tag">&lt;<span class="title">description</span>&gt;</span>Enable Hive's Table Lock Manager Service<span class="tag">&lt;/<span class="title">description</span>&gt;</span>
	<span class="tag">&lt;<span class="title">value</span>&gt;</span>true<span class="tag">&lt;/<span class="title">value</span>&gt;</span>
<span class="tag">&lt;/<span class="title">property</span>&gt;</span>

<span class="tag">&lt;<span class="title">property</span>&gt;</span>
	<span class="tag">&lt;<span class="title">name</span>&gt;</span>hive.zookeeper.quorum<span class="tag">&lt;/<span class="title">name</span>&gt;</span>
	<span class="tag">&lt;<span class="title">description</span>&gt;</span>Zookeeper quorum used by Hive's Table Lock Manager<span class="tag">&lt;/<span class="title">description</span>&gt;</span>
	<span class="tag">&lt;<span class="title">value</span>&gt;</span>localhost<span class="tag">&lt;/<span class="title">value</span>&gt;</span>
<span class="tag">&lt;/<span class="title">property</span>&gt;</span>
</pre></td></tr></table></figure>

<p>启动hive-server2服务</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre><span class="built_in">sudo</span> service hive-server2 start
</pre></td></tr></table></figure>

<p>连接上hive-server2看看</p>
<figure class="highlight config"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="code"><pre>[jojo@vm4 ~]$ beeline
Beeline version 0.12.0-cdh5.1.0 by Apache Hive
beeline&gt; !connect jdbc:hive2://localhost:10000 username password org.apache.hive.jdbc.HiveDriver
Connecting to jdbc:hive2://localhost:10000
Connected to: Apache Hive (version 0.12.0-cdh5.1.0)
Driver: Hive JDBC (version 0.12.0-cdh5.1.0)
Transaction isolation: TRANSACTION<span class="emphasis">_REPEATABLE_</span>READ
<span class="header">0: jdbc:hive2://localhost:10000&gt; SHOW TABLES;
+-----------+</span>
<span class="header">| tab_name  |
+-----------+</span>
<span class="code">+-----------+</span>
No rows selected (2.547 seconds)
0: jdbc:hive2://localhost:10000&gt; !quit
Closing: org.apache.hive.jdbc.HiveConnection
</pre></td></tr></table></figure><br>可以看到现在还没hive的数据表，在产生hive数据表之前，先要生成hive的数据目录<br><br><figure class="highlight bash"><table><tr><td class="gutter"><pre>1
2
</pre></td><td class="code"><pre><span class="built_in">sudo</span> -u hdfs hadoop fs -mkdir -p /user/hive/warehouse
<span class="built_in">sudo</span> -u hdfs hadoop fs -chmod -R <span class="number">1777</span> /user/hive/warehouse  <span class="comment">#让任何用户都能操作hive的数据</span>
</pre></td></tr></table></figure>

<p>下面来测试下hive</p>
<figure class="highlight config"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre></td><td class="code"><pre>0: jdbc:hive2://localhost:10000&gt; CREATE TABLE pokes (foo INT, bar STRING);  #产生一张表
No rows affected (1.166 seconds)
<span class="header">0: jdbc:hive2://localhost:10000&gt; show tables;
+-----------+</span>
<span class="header">| tab_name  |
+-----------+</span>
<span class="header">| pokes     |
+-----------+</span>
1 row selected (0.589 seconds)
<span class="header">0: jdbc:hive2://localhost:10000&gt; SELECT COUNT(*) FROM pokes;  #count下
+------+</span>
<span class="header">| _c0  |
+------+</span>
<span class="header">| 0    |
+------+</span>
1 row selected (56.58 seconds)

#检查下hdfs下刚才产生的表
[jojo@vm4 ~]$ sudo -u hdfs hadoop fs -ls -R /user/hive
drwxrwxrwt   - hdfs supergroup          0 2014-07-31 17:53 /user/hive/warehouse
drwxrwxrwt   - hive supergroup          0 2014-07-31 17:53 /user/hive/warehouse/pokes
</pre></td></tr></table></figure>

<p>到这里hive已经安装完成了,下面安装pig</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre><span class="built_in">sudo</span> yum install pig
</pre></td></tr></table></figure><br>pig安装完成啦~ 下面来测试下<br><br><figure class="highlight bash"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td><td class="code"><pre>pig 
grunt&gt; ls  <span class="comment">#列出当前用户目录下的文件</span>

grunt&gt; A = LOAD <span class="string">'input'</span>;  <span class="comment">#input目录下有hadoop的配置文件</span>
grunt&gt; B = FILTER A BY <span class="variable">$0</span> MATCHES <span class="string">'.*dfs[a-z.]+.*'</span>;   <span class="comment">#匹配dfs的项</span>
grunt&gt; DUMP B;  <span class="comment">#查看B的内容</span>

<span class="comment">#输出如下</span>
(    &lt;name&gt;dfs.replication&lt;/name&gt;)
(    &lt;name&gt;dfs.safemode.extension&lt;/name&gt;)
(     &lt;name&gt;dfs.safemode.min.datanodes&lt;/name&gt;)
(     &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;)
(     &lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt;)
(     &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;)
(    &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt;)
(    &lt;name&gt;dfs.client.file-block-storage-locations.timeout.millis&lt;/name&gt;)
(    &lt;name&gt;dfs.domain.socket.path&lt;/name&gt;)
(    &lt;name&gt;dfs.datanode.hdfs-blocks-metadata.enabled&lt;/name&gt;)
</pre></td></tr></table></figure>

<p>pig测试完成，到这里hadoop+hive+pig的开发环境配置完成。</p>
</div><footer class="post__foot u-cf"><ul class="post__tag u-fl"><li class="post__tag__item"><a href="/tags/hadoop/" class="post__tag__link">hadoop</a></li><li class="post__tag__item"><a href="/tags/hive/" class="post__tag__link">hive</a></li><li class="post__tag__item"><a href="/tags/pig/" class="post__tag__link">pig</a></li></ul><a href="/2014/07/31/hadoop-hive-pig-development-environment-setup/#disqus_thread" class="post__foot-link u-fr">0 COMMENTS</a></footer></article></main><footer class="foot"><div class="foot-copy u-fl">&copy; 2014 Yu Fang</div><menu class="page-menu u-fr"><li class="page-menu__item"><span title="Previous" class="page-menu__link icon-arrow-left page-menu__link--disabled"></span></li><li class="page-menu__item"><span title="Next" class="page-menu__link icon-arrow-right page-menu__link--disabled"></span></li></menu></footer><script>(function(h,g,l,k,j,i){j=h.createElement(g),i=h.getElementsByTagName(g)[0],
j.src="//"+l+".disqus.com/"+k+".js",i.parentNode.insertBefore(j,i)})
(document,"script","yeesblog","count");
</script><script>var disqus_shortname = "yeesblog";
(function () {
var s = document.createElement('script'); s.async = true;
s.type = 'text/javascript';
s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
(document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
}());
</script></body></html>